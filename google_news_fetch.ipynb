{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPONR3SIT4H9r7CPdE9Q7oh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Modified by Bravo based on https://github.com/iAhsanJaved/FetchGoogleNews/blob/master/main.py\n",
        "pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH3tdzQnM4PR",
        "outputId": "fcca3728-f8cd-44ed-81ed-911174a45b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m174.1/211.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.14.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=885930eddf6dc93b09bf17caa47efd9288c8c02c9229ba0d3e2d1cb8522afc55\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=8bce448224f9d4dd93ce662ceb3788da82e289ca1ab6ee970a1fb8893f1a0345\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=70dc52ab3a72a12790ad80bcc3206d4ed1632f5227a3c0ee0965e5498c9a7942\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=d06b7f1256d6a83b56e8262ea3d97aca8ac8002cd1d623ff130a03def4a1a345\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTybPdrZIDTj",
        "outputId": "5871cd0e-74db-4ea7-a21a-a77727a85431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your search term here: Apple\n",
            "Execution time 71.39073753356934\n"
          ]
        }
      ],
      "source": [
        "# import the needed libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time # for timing script\n",
        "import xml.etree.ElementTree as ET # built in library\n",
        "from datetime import datetime, timedelta\n",
        "from newspaper import Article\n",
        "\n",
        "def clean_url(search_term, data_filter):\n",
        "    \"\"\"\n",
        "    OUTPUT : url to be fecthed for the searched_item and data_filter\n",
        "     ---------------------------------------------------\n",
        "    Parameters:\n",
        "      today' - get headlines of the news that are released only in today\n",
        "                       'this_week' - get headlines of the news that are released in this week\n",
        "                       'this month' - news released in this month\n",
        "                       'this_year' - news released in this year\n",
        "                        number : int/str input for number of days ago\n",
        "                        or '' blank to get all data\n",
        "    \"\"\"\n",
        "    x = datetime.today()\n",
        "    today = str(x)[:10]\n",
        "    yesterday = str(x + timedelta(days=-1))[:10]\n",
        "    this_week = str(x + timedelta(days=-7))[:10]\n",
        "    if data_filter == 'today':\n",
        "        time = 'after%3A' + yesterday\n",
        "    elif data_filter == 'this_week':\n",
        "        time = 'after%3A'+ this_week + '+before%3A' + today\n",
        "    elif data_filter == 'this_year':\n",
        "        time = 'after%3A'+str(x.year - 1)\n",
        "    elif str(data_filter).isdigit():\n",
        "        temp_time = str(x + timedelta(days=-int(data_filter)))[:10]\n",
        "        time =  'after%3A'+ temp_time + '+before%3A' + today\n",
        "    else:\n",
        "        time=''\n",
        "    url = f'https://news.google.com/rss/search?q={search_term}+'+time+'&hl=en-US&gl=US&ceid=US%3Aen'\n",
        "    return url\n",
        "\n",
        "# clear the description\n",
        "def get_text(x):\n",
        "    start = x.find('<p>')+3\n",
        "    end = x.find('</p>')\n",
        "    return x[start:end]\n",
        "\n",
        "def get_content(url):\n",
        "    \"\"\"\n",
        "    Extract the main content of a news article from its URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def get_news(search_term, data_filter=None):\n",
        "    \"\"\"\n",
        "    Search through Google News with the \"search_term\" and get the headlines\n",
        "     and the contents of the news that was released today, this week, this month,\n",
        "    or this year (\"date_filter\").\n",
        "    \"\"\"\n",
        "\n",
        "    url = clean_url(search_term, data_filter)\n",
        "    response = requests.get(url)\n",
        "    # get the root directly as we have text file of string now\n",
        "    root = ET.fromstring(response.text)\n",
        "    #get the required data\n",
        "    title = [i.text for i in root.findall('.//channel/item/title') ]\n",
        "    link = [i.text for i in root.findall('.//channel/item/link') ]\n",
        "    description = [i.text for i in root.findall('.//channel/item/description') ]\n",
        "    pubDate = [i.text for i in root.findall('.//channel/item/pubDate') ]\n",
        "    source = [i.text for i in root.findall('.//channel/item/source') ]\n",
        "    # clear the description\n",
        "    short_description = list(map(get_text, description))\n",
        "\n",
        "    # extract content for each article\n",
        "    content = [get_content(url) for url in link]\n",
        "\n",
        "    # set the data frame\n",
        "    df = pd.DataFrame({'title': title, 'link': link, 'description': short_description, 'date': pubDate, 'source': source, 'content': content})\n",
        "    # adjust the date column\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    # for saving purpose uncomment the below\n",
        "    df.to_csv(f'{search_term}_news.csv', encoding='utf-8-sig', index=False)\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    search_term = str(input('Enter your search term here: '))\n",
        "    data = get_news(search_term, data_filter=5)\n",
        "    end = time.time() - start\n",
        "    print(\"Execution time\", end)\n"
      ]
    }
  ]
}